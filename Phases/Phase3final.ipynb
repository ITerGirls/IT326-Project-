{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Clustering\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "One of the primary objectives of the project is to utilize clustering techniques on our\ndataset in order to identify the class for each row.\n\nTo begin with, we will read the data and exclude the class labels, as clustering is an \nunsupervised learning approach that does not rely on predefined classes.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Load the dataset\ndf = pd.read_csv('dataPreprocessing.csv')\nprint(df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'pandas.errors.ParserError'>",
          "evalue": "Error tokenizing data. C error: Expected 1 fields in line 670, saw 7\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataPreprocessing.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 670, saw 7\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": "df=pd.read_csv('dataPreprocessing.csv')\nfeatures=df.drop('discretized_time_spent',axis=1\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "unterminated string literal (detected at line 2) (<ipython-input-19-784853474cfd>, line 2)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    features=df.drop('discretized_time_spent',axis=1\")\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": "# Define categorical and numerical columns\ncategorical_cols = ['platform', 'location', 'demographics', 'profession']\nnumerical_cols = ['age', 'time_spent', 'income', 'indebt', 'isHomeOwner', 'Owns_Car']\n\n# Create a preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', OneHotEncoder(), categorical_cols)\n    ])\n\n# Fit and transform the data\nscaler = StandardScaler()\nscaled_features = preprocessor.fit_transform(df)\n\n# Create a new DataFrame with the scaled features \nencoded_cols = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\ndf_scaled  = pd.DataFrame(scaled_features, columns=numerical_cols + list(encoded_cols))\n\n# Display the scaled DataFrame\nprint(\"\\nScaled DataFrame: \")\nprint(df.head())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Fit and transform the data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 14\u001b[0m scaled_features \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame with the scaled features \u001b[39;00m\n\u001b[1;32m     17\u001b[0m encoded_cols \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mnamed_transformers_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget_feature_names_out(categorical_cols)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": " k-means clustering with k=3, k=5, and k=8.\n\nk=3: General segmentation.\n\nk=5: More specific segmentation.\n\nk=8: Detailed segmentation.\n\nThese values balance simplicity and detail, providing insights into user engagement based on average time spent.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np \nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import scale\n# set a seed for a random number generation \nnp.random.seed(8953)\n\n# determining K values  \nk_values = [3 , 5, 8]\n\ncluster_centers = {}\ncluster_labels = {}\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=8953)\n    kmeans_result = kmeans.fit(df_scaled )\n    \n    cluster_centers[k] = kmeans_result.cluster_centers_\n    cluster_labels[k] = kmeans_result.labels_\n\n    print(f\"Cluster Centers (K={k}):\")\n    print(cluster_centers[k])\n    print(\"\\nCluster Labels:\")\n    print(cluster_labels[k])\n    print()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'df_scaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[1;32m     14\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8953\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     kmeans_result \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdf_scaled\u001b[49m )\n\u001b[1;32m     17\u001b[0m     cluster_centers[k] \u001b[38;5;241m=\u001b[39m kmeans_result\u001b[38;5;241m.\u001b[39mcluster_centers_\n\u001b[1;32m     18\u001b[0m     cluster_labels[k] \u001b[38;5;241m=\u001b[39m kmeans_result\u001b[38;5;241m.\u001b[39mlabels_\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_scaled' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": "To assess and display the outcome from the previous calculation using the Silhouette coefficient.\n\n\n\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 1. Using silhouette coefficient to visulize the results",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%pip install yellowbrick \nfrom yellowbrick.cluster import SilhouetteVisualizer\n\n\n# determining  K values  \nk_values = [3 , 5, 8]\n\n\ncluster_centers = {}\ncluster_labels = {}\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=8953)\n    kmeans_result = kmeans.fit(df_scaled)\n    \n    # Create and fit SilhouetteVisualizer\n    visualizer = SilhouetteVisualizer(kmeans, colors='yellowbrick')\n    visualizer.fit(df_scaled)\n    visualizer.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'df_scaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[1;32m     13\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8953\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     kmeans_result \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdf_scaled\u001b[49m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Create and fit SilhouetteVisualizer\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     visualizer \u001b[38;5;241m=\u001b[39m SilhouetteVisualizer(kmeans, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myellowbrick\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_scaled' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": "The dotted line representing the silhouette coefficient value assists us in determining the optimal number\nof clusters. A higher silhouette coefficient value indicates a better fit for the number of clusters, K.\nFor instance: \nWhen K=3, the silhouette coefficient value is 0.11.\n\nWhen K=5, it increases to 0.13, \n\nwhen K=8, it further improves to 0.14.\n\nThis suggests that the optimal choice is K=8.                                                                                                    \nThe second evaluation method used is Elbow method where the best number of cluster is the turning point in the curve.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "1. *k=3:* This choice allows for a basic segmentation of users into three groups based on their average time spent. It provides a high-level overview of user engagement levels, which can be useful for general marketing strategies or product development.\n\n2. *k=5:* Selecting k=5 provides a more detailed segmentation, allowing for finer distinctions between user groups. This can be beneficial for targeted marketing campaigns or personalized recommendations, as it identifies additional nuances in user behavior based on average time spent.\n\n3. *k=8:* Choosing k=8 offers even greater granularity, enabling the identification of more specific user segments with distinct patterns of average time spent. This level of detail can be valuable for understanding complex user behaviors and tailoring strategies to different user segments.\n\nBy selecting these three values of k, we aim to balance simplicity and granularity in the clustering analysis. This approach allows for a range of insights, from high-level trends to detailed user segmentations, providing a comprehensive view of user engagement based on average time spent.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 2. Using Elbow method to Visulize the results",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize an empty list to store WCSS values\nwcss = []\n\n# Define a range of k values\nk_values = range(1, 11)\n\n# Calculate WCSS for each k\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, random_state=8953)\n    kmeans.fit(df_scaled)\n    wcss.append(kmeans.inertia_)\n\n# Plot the elbow method graph\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('WCSS')\n\n# Annotate the elbow point with a red dot and line\noptimal_k = 3  \nplt.scatter(optimal_k, wcss[optimal_k - 1], c='red', marker='o', label='Optimal k')\nplt.axvline(x=optimal_k, color='red', linestyle='--', label='Optimal k')\nplt.legend()\n\nplt.show()",
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'df_scaled' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m     14\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mi, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8953\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     kmeans\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdf_scaled\u001b[49m)\n\u001b[1;32m     16\u001b[0m     wcss\u001b[38;5;241m.\u001b[39mappend(kmeans\u001b[38;5;241m.\u001b[39minertia_)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Plot the elbow method graph\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_scaled' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": "Upon visually examining the graph illustrating the total within-cluster sum of squares (WSS) in relation to K,\nit becomes apparent that the inflection point aligns with a cluster number of 2.\nThis suggests that the ideal number of clusters is 2.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Classification",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "in classification When constructing a Decision Tree, the Gini Index and Entropy measures are used to determine which features to use for splitting the data at each node (point in the tree). The feature that results in the least reduction in Gini Index or Entropy is chosen, indicating that it splits the data better and achieves a more accurate classification.\n\n   Three partition sizes:\n   \n   90% training, 10% testing\n\n   80% training, 20% testing\n\n   70% training, 30% testing",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Gini index\n\r\nThe Gini Index is a measure used to quantify the deviation or gap between different classes within a dataset. It ranges from 0 to 1, where a value of 0 indicates no deviation or variation between classes, while a value of 1 indicates a significant deviation and variation between classes\n# Entropy.\r\n\r\nEntropy, on the other hand, is a measure used to quantify the disorder or chaos within a dataset. It also ranges from 0 to 1, where a value of 0 indicates no chaos or lack of order, while a value of 1 indicates high chaos or lack of ordication.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\nfrom sklearn import tree",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": "### 90% training, 10% testing\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#convert categorical variables into numerical form\nlabel_encoder = LabelEncoder()\ndf_encoded = df.apply(lambda col: label_encoder.fit_transform(col) if col.dtype == \"object\" else col)\n\n#drop the class\nX = df_encoded.drop(columns=[\"discretized_time_spent\"])\ny = df_encoded[\"discretized_time_spent\"]\n\n#Spilt the data\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n\n\nclf = DecisionTreeClassifier()\nclf.fit(x_train, y_train)\n\n\ny_pred = clf.predict(x_test)\n\n# To measure Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n#Compute confusion matrix to evaluate the accuracy of a classification.\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Sports','Travel','Lifestyle'])\ndisp.plot(cmap=plt.cm.Reds)\nplt.show()\n\n\nplt.figure(figsize=(20, 10))\nplot_tree(clf, feature_names=X.columns, class_names=['Sports','Travel','Lifestyle'], filled=True)\nplt.show()\n\n#Spilt by entropy\nclf_entropy = DecisionTreeClassifier(criterion=\"entropy\")\nclf_entropy.fit(x_train, y_train)\n\n\ny_pred_entropy = clf_entropy.predict(x_test)\n\n\naccuracy_entropy = accuracy_score(y_test, y_pred_entropy)\nprint(\"Accuracy (Entropy):\", accuracy_entropy)\n\n\nplt.figure(figsize=(20, 10))\nplot_tree(clf_entropy, feature_names=X.columns, class_names=['Sports','Travel','Lifestyle'], filled=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#convert categorical variables into numerical form\u001b[39;00m\n\u001b[1;32m      2\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m----> 3\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m col: label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(col) \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m col)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#drop the class\u001b[39;00m\n\u001b[1;32m      6\u001b[0m X \u001b[38;5;241m=\u001b[39m df_encoded\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscretized_time_spent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": "### 80% training, 20% testing",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "label_encoder = LabelEncoder()\ndf_encoded = df.apply(lambda col: label_encoder.fit_transform(col) if col.dtype == \"object\" else col)\n\n\nX = df_encoded.drop(columns=[\"discretized_time_spent\"])\ny = df_encoded[\"discretized_time_spent\"]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n\nclf = DecisionTreeClassifier()\nclf.fit(x_train, y_train)\n\n\ny_pred = clf.predict(x_test)\n\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Sports','Travel','Lifestyle'])\ndisp.plot(cmap=plt.cm.Reds)\nplt.show()\n\n\nplt.figure(figsize=(20, 10))\nplot_tree(clf, feature_names=X.columns, class_names=['Sports','Travel','Lifestyle'], filled=True)\nplt.show()\n\n\nclf_entropy = DecisionTreeClassifier(criterion=\"entropy\")\nclf_entropy.fit(x_train, y_train)\n\n\ny_pred_entropy = clf_entropy.predict(x_test)\n\n\naccuracy_entropy = accuracy_score(y_test, y_pred_entropy)\nprint(\"Accuracy (Entropy):\", accuracy_entropy)\n\n\nplt.figure(figsize=(20, 10))\nplot_tree(clf_entropy, feature_names=X.columns, class_names=['Sports','Travel','Lifestyle'], filled=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m----> 2\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m col: label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(col) \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m col)\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m df_encoded\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscretized_time_spent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m df_encoded[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscretized_time_spent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "### 70% training, 30% testing\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": "label_encoder = LabelEncoder()\ndf_encoded = df.apply(lambda col: label_encoder.fit_transform(col) if col.dtype == \"object\" else col)\n\n\nX = df_encoded.drop(columns=[\"discretized_time_spent\"])\ny = df_encoded[\"discretized_time_spent\"]\n\n#70% training, 30% testing\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n\nclf = DecisionTreeClassifier()\nclf.fit(x_train, y_train)\n\n\ny_pred = clf.predict(x_test)\n\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Sports','Travel','Lifestyle'])\ndisp.plot(cmap=plt.cm.Reds)\nplt.show()\n\n\nplt.figure(figsize=(20, 10))\nplot_tree(clf, feature_names=X.columns, class_names=['Sports','Travel','Lifestyle'], filled=True)\nplt.show()\n\n\nclf_entropy = DecisionTreeClassifier(criterion=\"entropy\")\nclf_entropy.fit(x_train, y_train)\n\n\ny_pred_entropy = clf_entropy.predict(x_test)\n\n\naccuracy_entropy = accuracy_score(y_test, y_pred_entropy)\nprint(\"Accuracy (Entropy):\", accuracy_entropy)\n\n\nplt.figure(figsize=(20, 10))\nplot_tree(clf_entropy, feature_names=X.columns, class_names=['Sports','Travel','Lifestyle'], filled=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m----> 2\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m col: label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(col) \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m col)\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m df_encoded\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscretized_time_spent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m df_encoded[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscretized_time_spent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "  partition_sizes = [0.1, 0.2, 0.3]\n\n  criterions = [\"entropy\", \"gini\"]\n\n  results = {}\n\n  for size in partition_sizes:\n      for criterion in criterions:\n        \n          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-size, random_state=42)\n\n          clf = DecisionTreeClassifier(criterion=criterion, random_state=42)\n        \n          clf.fit(X_train, y_train)\n        \n          y_pred = clf.predict(X_test)\n        \n          accuracy = accuracy_score(y_test, y_pred)\n        \n          results[(size, criterion)] = accuracy\n\n  for (size, criterion), accuracy in results.items():\n      print(f\"Partition Size: {size}, Criterion: {criterion}, Accuracy: {accuracy}\")\n\n  sizes_entropy = [size for size, criterion in results if criterion == \"entropy\"]\n  accuracies_entropy = [results[(size, \"entropy\")] for size in sizes_entropy]\n  sizes_gini = [size for size, criterion in results if criterion == \"gini\"]\n  accuracies_gini = [results[(size, \"gini\")] for size in sizes_gini]\n\n  plt.figure(figsize=(10, 6))\n  plt.plot(sizes_entropy, accuracies_entropy, marker='o', label='Entropy')\n  plt.plot(sizes_gini, accuracies_gini, marker='o', label='Gini')\n  plt.title('Accuracy vs Partition Size')\n  plt.xlabel('Partition Size')\n  plt.ylabel('Accuracy')\n  plt.legend()\n  plt.grid(True)\n  plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m partition_sizes:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m criterion \u001b[38;5;129;01min\u001b[39;00m criterions:\n\u001b[0;32m---> 10\u001b[0m         X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msize, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     12\u001b[0m         clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(criterion\u001b[38;5;241m=\u001b[39mcriterion, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     14\u001b[0m         clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": "#The results that were printed indicate the achieved accuracy for each different combination of test ratio and criterion type. Here is an explanation for each row of results:\r\n\r\n1. Partition Size: 0.1, Criterion: entropy, Accuracy: 0.34053156146179403\r\n   - This result indicates that when using a test ratio of 0.1 and using the \"entropy\" criterion, an accuracy of 0.34053156146179403 was achieved.\r\n\r\n2. Partition Size: 0.1, Criterion: gini, Accuracy: 0.33056478405315615\r\n   - This result indicates that when using a test ratio of 0.1 and using the \"gini\" criterion, an accuracy of 0.33056478405315615 was achieved.\r\n\r\n3. Partition Size: 0.2, Criterion: entropy, Accuracy: 0.32710280373831774\r\n   - This result indicates that when using a test ratio of 0.2 and using the \"entropy\" criterion, an accuracy of 0.32710280373831774 was achieved.\r\n\r\n4. Partition Size: 0.2, Criterion: gini, Accuracy: 0.3401869158878505\r\n   - This result indicates that when using a test ratio of 0.2 and using the \"gini\" criterion, an accuracy of 0.3401869158878505 was achieved.\r\n\r\n5. Partition Size: 0.3, Criterion: entropy, Accuracy: 0.31837606837606836\r\n   - This result indicates that when using a test ratio of 0.3 and using the \"entropy\" criterion, an accuracy of 0.31837606837606836 was achieved.\r\n\r\n6. Partition Size: 0.3, Criterion: gini, Accuracy: 0.33547008547008544\r\n   - This result indicates that when using a test ratio of 0.3 and using the \"gini\" criterion, an accuracy of 0.33547008547008544 was achieved.\r\n\r\nOverall, it can be observed that the accuracy varies across the different combinations of test ratio and criterion type. These results can be used to choose the most suitable combination for achieving the best performance in data classification based on accuracy.",
      "metadata": {}
    }
  ]
}